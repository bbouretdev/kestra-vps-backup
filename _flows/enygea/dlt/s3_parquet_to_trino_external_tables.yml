id: s3_parquet_to_trino_external_tables
namespace: enygea.dlt

inputs:
  - id: trino_catalog
    type: STRING
    defaults: hive_json_scw

tasks:
# ============================================================
# 1️⃣ PYTHON — Scan S3 + inférence + génération SQL
# ============================================================
- id: generate_sql
  type: io.kestra.plugin.scripts.python.Script
  taskRunner:
    type: io.kestra.plugin.scripts.runner.docker.Docker
  containerImage: python:3.11-slim
  beforeCommands:
    - pip install duckdb pyarrow boto3
  env:
    AWS_ACCESS_KEY_ID: SCWB3XMCCXHTNQC5KQQH
    AWS_SECRET_ACCESS_KEY: 764e1512-586c-4cbd-ad83-8251a0d27b7c
  outputFiles:
    - schemas.json
    - tables.json
  script: |
    import duckdb
    import boto3
    import os
    import json
    from collections import defaultdict

    BUCKET = "sandbox-datalake-bronze"
    PREFIX = "bronze_dlt"
    ENDPOINT = "https://s3.fr-par.scw.cloud"

    TRINO_CATALOG = "{{ inputs.trino_catalog }}"

    # -------------------------
    # S3 LIST
    # -------------------------
    s3 = boto3.client(
        "s3",
        endpoint_url=ENDPOINT,
        aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],
        aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"],
    )

    paginator = s3.get_paginator("list_objects_v2")

    folders = defaultdict(list)

    for page in paginator.paginate(Bucket=BUCKET, Prefix=PREFIX):
        for obj in page.get("Contents", []):
            key = obj["Key"]
            if key.endswith(".parquet"):
                folder = key.rsplit("/", 1)[0]
                folders[folder].append(key)

    if not folders:
        print("No parquet found, exiting.")
        exit(0)

    # -------------------------
    # DUCKDB CONFIG
    # -------------------------
    con = duckdb.connect(database=":memory:")
    con.execute("INSTALL httpfs;")
    con.execute("LOAD httpfs;")

    con.execute(f"""
    SET s3_endpoint='s3.fr-par.scw.cloud';
    SET s3_url_style='path';
    SET s3_use_ssl=true;
    SET s3_region='fr-par';
    SET s3_access_key_id='{os.environ["AWS_ACCESS_KEY_ID"]}';
    SET s3_secret_access_key='{os.environ["AWS_SECRET_ACCESS_KEY"]}';
    """)

    def duckdb_to_trino(t):
        t = t.upper()
        if t.startswith(("VARCHAR", "STRING")): return "VARCHAR"
        if t.startswith(("BIGINT", "INTEGER")): return "BIGINT"
        if t.startswith(("DOUBLE", "FLOAT")): return "DOUBLE"
        if t.startswith("BOOLEAN"): return "BOOLEAN"
        if t.startswith("TIMESTAMP"): return "TIMESTAMP"
        if t.startswith("DATE"): return "DATE"
        if t.startswith(("STRUCT", "MAP", "LIST")): return "JSON"
        if t.startswith("DECIMAL"): return "DECIMAL"
        return "VARCHAR"

    schema_sql = set()
    table_sql = []

    # -------------------------
    # PER FOLDER
    # -------------------------
    for folder, files in folders.items():
        trino_schema = folder.split("/")[-2]
        trino_table = folder.split("/")[-1]

        parquet_path = f"s3://{BUCKET}/{folder}/*.parquet"

        schema = con.execute(
            f"DESCRIBE SELECT * FROM read_parquet('{parquet_path}')"
        ).fetchall()

        columns_sql = ",\n    ".join(
          f'"{col}" {duckdb_to_trino(dtype)}'
          for col, dtype, *_ in schema
        )

        schema_sql.add(
            f"""
            CREATE SCHEMA IF NOT EXISTS {TRINO_CATALOG}.{trino_schema}
            WITH (location = 's3a://{BUCKET}/bronze_dlt/{trino_schema}/')
            """.strip()
        )

        table_sql.append(
            f"""
            CREATE TABLE IF NOT EXISTS {TRINO_CATALOG}.{trino_schema}.{trino_table} (
                {columns_sql}
            )
            WITH (
                format = 'PARQUET',
                external_location = 's3a://{BUCKET}/{folder}'
            )
            """.strip()
        )

    with open("schemas.json", "w") as f:
        json.dump(list(schema_sql), f)

    with open("tables.json", "w") as f:
        json.dump(table_sql, f)

# ============================================================
# 2️⃣ CREATE SCHEMAS (1 statement / task)
# ============================================================
- id: create_schemas
  type: io.kestra.plugin.core.flow.ForEach
  concurrencyLimit: 1
  values: "{{ read(outputs.generate_sql.outputFiles['schemas.json']) }}"
  tasks:
    - id: exec_schema
      type: io.kestra.plugin.jdbc.trino.Query
      url: jdbc:trino://trino.davidson-si-nord.fr:443
      username: Davidson
      sql: "{{ taskrun.value }}"

# ============================================================
# 3️⃣ CREATE TABLES (1 statement / task)
# ============================================================
- id: create_tables
  type: io.kestra.plugin.core.flow.ForEach
  values: "{{ read(outputs.generate_sql.outputFiles['tables.json']) }}"
  concurrencyLimit: 1
  tasks:
    - id: exec_table
      type: io.kestra.plugin.jdbc.trino.Query
      url: jdbc:trino://trino.davidson-si-nord.fr:443
      username: Davidson
      sql: "{{ taskrun.value }}"