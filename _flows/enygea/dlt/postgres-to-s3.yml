id: postgres-to-s3
namespace: enygea.dlt

tasks:
  - id: dlt_pipeline
    type: io.kestra.plugin.core.flow.WorkingDirectory
    tasks:
    - id: clone_repo
      type: io.kestra.plugin.git.Clone
      url: https://github.com/bbouretdev/enygea_poc_dlt.git
      branch: main
  
    - id: run_pipeline
      type: io.kestra.plugin.scripts.python.Script
      taskRunner:
        type: io.kestra.plugin.scripts.runner.docker.Docker
      containerImage: python:3.11-slim
      beforeCommands:
        - pip install --upgrade pip
        - pip install "dlt[postgres,parquet,filesystem,s3,sql_database]" pandas psycopg2-binary
      env:
        POSTGRES_HOST: "4505da19-4cd7-40c8-ab0a-d7f44b75731e.pg.sdb.fr-par.scw.cloud"
        POSTGRES_PORT: "5432"
        POSTGRES_DB: "serverless-sqldb-datasonar"
        POSTGRES_USER: "be52a34e-7113-44fd-805a-829d8faaaa5b"
        POSTGRES_PASSWORD: "{{ secret('SCALEWAY_DB_PASSWORD') }}"
        AWS_ACCESS_KEY_ID: "{{ secret('AWS_ACCESS_KEY_ID') }}"
        AWS_SECRET_ACCESS_KEY: "{{ secret('AWS_SECRET_ACCESS_KEY') }}"
        DESTINATION__FILESYSTEM__BUCKET_URL: "s3://enygea-bronze/dltX"
      script: |
        import subprocess
        subprocess.run(["python", "pipelines/database/postgres/postgres-test.py"], check=True)